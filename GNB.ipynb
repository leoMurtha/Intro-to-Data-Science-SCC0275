{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "Naive Bayes is a classical ML algorithm used for text analytics and general classification.\n",
    "The following implementation describes the Gaussian Naive Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Algorithm\n",
    "The algorithm it is very statistical based using prior and posterior probabilities of the classes in the dataset.\n",
    "\n",
    "Using the Bayes' Theorem below as the main idea:\n",
    "\n",
    "$$P(A | B)P(B) = P(A \\cap B) = P(B \\cap A) = P(B | A)P(A)$$\n",
    "\n",
    "$$P(A | B) = \\frac{P(B | A)P(A)}{P(B)}$$\n",
    "\n",
    "\n",
    "Now using the theorem we can ask what is the probability of a given class given that a specific document happened.\n",
    "\n",
    "$$p(\\text{class} \\mid \\mathbf {\\text{data}} )={\\frac {p(\\mathbf {\\text{data}} \\mid \\text{class}) * p(\\text{class})}{p(\\mathbf {\\text{data}} )}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "\n",
    "* $p(class | data)$ is called the posterior.\n",
    "* $p(data | class)$ is called the likelihood.\n",
    "* $p(class)$ is called the prior.\n",
    "* $p(data)$ is called the marginal probability.\n",
    "\n",
    "\n",
    "The equation above describes the full Bayes algorithm but some probabilites are very non pratical to calculate an example of this is the $P(data)$ because if we have a never seen document in the dataset this $P(data)$ is going to be $0$, so usually we don't calculate the marginal probabilities in real word cases.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**The term Naive comes from assuming that the variables are independent of each other when they may not be**\n",
    "\n",
    "Then using the independance factor the Bayes' Theorem can be calculated as below:\n",
    "<div>\n",
    "<img src=\"images/BayesSimple.png\" width=\"600\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "In this notebook I'm going to approach the 20 News Group dataset, the steps to solve/classify this dataset are pre-processing the text data, training the Gaussian Bayes Classifier and testing it with unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "* converting all letters to lower or upper case\n",
    "* converting numbers into words or removing numbers\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing white spaces\n",
    "* expanding abbreviations\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data using sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_data = fetch_20newsgroups(subset='train', shuffle=True)\n",
    "test_data = fetch_20newsgroups(subset='test', shuffle=True)\n",
    "\n",
    "#train.data: holds the text data\n",
    "#train.target: holds the id's for the classes\n",
    "#train.target_names: holds the class string names\n",
    "#train.filenames: holds the filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps\n",
    "\n",
    "* converting all letters to lower or upper case\n",
    "* converting numbers into words or removing numbers\n",
    "* removing emails\n",
    "* removing punctuations, accent marks and other diacritics\n",
    "* removing white spaces\n",
    "* expanding abbreviations\n",
    "\n",
    "* removing stop words, sparse terms, and particular words\n",
    "* text canonicalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "punctuation = \"\"\"\\!\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?[\\]\\^\\_\\`\\{\\|\\}~\"\"\"\n",
    "\n",
    "# Defining cleaning regexes\n",
    "number_re = re.compile(r'(\\d+)', re.I | re.M | re.U)\n",
    "punkt_re = re.compile(r'([%s])' % punctuation, re.I | re.M | re.U)\n",
    "email_re = re.compile(r'(\\w+\\@\\w+)', re.I | re.M | re.U)\n",
    "whitespaces_re = re.compile(r'(\\s)', re.I | re.M | re.U)\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    if type(doc) != str:\n",
    "        raise Exception('Doc is not text data')\n",
    "\n",
    "    # Making a copy of the original document\n",
    "    _doc = doc\n",
    "\n",
    "    # Stripping\n",
    "    _doc = _doc.strip()\n",
    "\n",
    "    # lower case\n",
    "    _doc = _doc.lower()\n",
    "\n",
    "    # removing numbers\n",
    "    _doc = number_re.sub('', _doc)\n",
    "\n",
    "    # removing punkt before email so the email regex is simpler\n",
    "    _doc = punkt_re.sub('', _doc)\n",
    "\n",
    "    # removing emails\n",
    "    _doc = email_re.sub('', _doc)\n",
    "\n",
    "    # removing long white spaces to single spacew\n",
    "    _doc = whitespaces_re.sub(' ', _doc)\n",
    "\n",
    "    # Removing stopwords and words that aren't four units long\n",
    "    tokens = [\n",
    "        token for token in word_tokenize(_doc)\n",
    "        if (token not in stop_words and len(token) > 4 and '\\\\' not in token)\n",
    "    ]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes(Gaussian) Implementation\n",
    "\n",
    "Following sections will be about the implementation of the GNB for text classification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "In the training process we are going to receive the non cleaned text data and preprocess it and build the dictionary and calculate the prior probabilities and the likelihood probabilities also all of this using the Gaussian distribution to solve the zero frequency problem assuming variable independance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class GNB:\n",
    "    _dict = set()\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def dictionary(self, docs):\n",
    "        # Building or dictionary with the entire training dataset\n",
    "        [self._dict.update(preprocess(doc)) for doc in docs]\n",
    "\n",
    "        print('Dictionary length %d\\n' % (len(self._dict)))\n",
    "\n",
    "    # Calculates p(x | y), p of x given that y happened using an gaussian distribution\n",
    "    def likelihood_prob(self, x, mean_y, var_y):\n",
    "        \n",
    "        # Probability density function\n",
    "        exponent = math.exp(-(math.pow(x-mean_y,2)/(2*math.pow(var_y,2))))\n",
    "        \n",
    "        return (1 / (math.sqrt(2*math.pi) * var_y)) * exponent\n",
    "        #p_x_y = (1 / np.sqrt(2 * np.pi * var_y)) * np.exp(\n",
    "        #    ((mean_y - x)**2) / 2 * var_y)\n",
    "        \n",
    "        #return p_x_y\n",
    "\n",
    "    def prior_probs(self, X):\n",
    "        counts = dict(zip(*np.unique(X, return_counts=True)))\n",
    "\n",
    "        total = len(X)\n",
    "\n",
    "        prior = np.array([counts[i] / total for i in range(len(counts))])\n",
    "\n",
    "        return prior\n",
    "    \n",
    "    def create_table(self, X, _dict):\n",
    "        # Creating the frequency table\n",
    "        nrows = len(X.data)\n",
    "        freq = {key: np.zeros(nrows) for key in self._dict}\n",
    "\n",
    "        for doc, i in zip(X.data, range(nrows)):\n",
    "            if(i % 1000 == 0):\n",
    "                print('Doc [%d] out of [%d]' % (i, nrows))\n",
    "\n",
    "            # Preprocessing the data\n",
    "            words = preprocess(doc)\n",
    "\n",
    "            for w in words:\n",
    "                if w not in _dict:\n",
    "                    continue\n",
    "                # Getting the vector assigned by the word w\n",
    "                vec = freq[w]\n",
    "\n",
    "                # In the ith position (observation id) sum one of ocurrence\n",
    "                vec[i] += 1\n",
    "\n",
    "        return freq\n",
    "    \n",
    "    def train(self, X):\n",
    "        # Creating the dictionary\n",
    "        self.dictionary(X.data)\n",
    "\n",
    "        # Calculating the class prior probabilities\n",
    "        self.p_class = self.prior_probs(X.target)\n",
    "        \n",
    "        self.freq = self.create_table(X, self._dict)\n",
    "        \n",
    "        # Assigning the classes\n",
    "        self.freq.update({'class': X.target})\n",
    "\n",
    "        # class       w1                w2  w3  w4 w5 ...\n",
    "        # 0     (mean_w1, var_w1)\n",
    "        # 1\n",
    "        # 2\n",
    "        # 3\n",
    "\n",
    "                \n",
    "    def test(self, X):\n",
    "        \n",
    "        test_freq = self.create_table(X, self._dict)\n",
    "        \n",
    "        correct = 0\n",
    "        p_class_data = np.zeros(len(X.target_names))\n",
    "        \n",
    "        \n",
    "        for target, i in zip(X.target[:3], range(len(X.data[:3]))):\n",
    "            if(i % 1000 == 0):\n",
    "                print('Test doc [%d] out of [%d]' % (i, 3))\n",
    "                \n",
    "\n",
    "            for cls in range(len(X.target_names)):\n",
    "                likelihood = 1\n",
    "                for key, vec in self.freq.items():\n",
    "                    test_vec = test_freq[key]                 \n",
    "                    group = vec[np.where(self.freq['class'] == cls)]\n",
    "                    \n",
    "                    mean_y = group.mean()\n",
    "                    var_y = group.var()\n",
    "                    if test_vec[i] != 0 and mean_y !=0 and var_y != 0:\n",
    "                            \n",
    "                        p_l = self.likelihood_prob(x=test_vec[i], mean_y=mean_y, var_y=var_y)\n",
    "                        \n",
    "                        likelihood = likelihood*p_l\n",
    "                        \n",
    "                p_class_data[cls] = self.p_class[cls]*likelihood\n",
    "        \n",
    "            \n",
    "            \n",
    "            if(np.argmax(p_class_data) == target):\n",
    "                correct += 1\n",
    "                \n",
    "            print(np.argmax(p_class_data), target)\n",
    "            \n",
    "            print('Accuracy %f' % (correct / (i+1)))\n",
    "            \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary length 85113\n",
      "\n",
      "Doc [0] out of [11314]\n",
      "Doc [1000] out of [11314]\n",
      "Doc [2000] out of [11314]\n",
      "Doc [3000] out of [11314]\n",
      "Doc [4000] out of [11314]\n",
      "Doc [5000] out of [11314]\n",
      "Doc [6000] out of [11314]\n",
      "Doc [7000] out of [11314]\n",
      "Doc [8000] out of [11314]\n",
      "Doc [9000] out of [11314]\n",
      "Doc [10000] out of [11314]\n",
      "Doc [11000] out of [11314]\n",
      "Doc [0] out of [7532]\n",
      "Doc [1000] out of [7532]\n",
      "Doc [2000] out of [7532]\n",
      "Doc [3000] out of [7532]\n",
      "Doc [4000] out of [7532]\n",
      "Doc [5000] out of [7532]\n",
      "Doc [6000] out of [7532]\n",
      "Doc [7000] out of [7532]\n",
      "Test doc [0] out of [3]\n",
      "0 7\n",
      "Accuracy 0.000000\n",
      "0 5\n",
      "Accuracy 0.000000\n",
      "0 0\n",
      "Accuracy 0.333333\n"
     ]
    }
   ],
   "source": [
    "classifier = GNB()\n",
    "\n",
    "\n",
    "classifier.train(train_data)\n",
    "\n",
    "classifier.test(test_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
